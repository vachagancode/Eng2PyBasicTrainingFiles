{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPPr2ggq940F"
      },
      "source": [
        "# This notebook is just for training and saving the model in google colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rmdir .ipynb_checkpoints/\n",
        "!rmdir CoNaLa/.ipynb_checkpoints/"
      ],
      "metadata": {
        "id": "g6dSlOGV_BE4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "\tdef __init__(self, d_model : int, vocab_size : int):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.d_model = d_model\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.d_model)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\tdef __init__(self, d_model : int, max_length : int):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.d_model = d_model\n",
        "\t\tself.max_length = max_length\n",
        "\n",
        "\t\tpe = torch.zeros(self.max_length, self.d_model)\n",
        "\t\tposition = torch.arange(0,  self.max_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "\t\tdiv = torch.exp(-1 * (torch.arange(0, d_model, 2) / d_model) * math.log(10000))\n",
        "\n",
        "\t\tpe[:, 0::2] = torch.sin(position * div)\n",
        "\t\tpe[:, 1::2] = torch.cos(position * div)\n",
        "\n",
        "\t\tself.register_buffer(\"pe\", pe) # `pe` will be used but not be trained\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn x + self.pe[:x.size(1)].unsqueeze(0)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\tdef __init__(self, d_model : int, num_heads : int, max_length : int, dropout : float):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.d_model = d_model\n",
        "\t\tself.num_heads = num_heads\n",
        "\t\tself.max_length = max_length\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\t\tassert d_model % num_heads == 0, \"Model embedding dimension (d_model) must be divisible by the number of heads(num_heads)\"\n",
        "\t\tself.d_k = self.d_model // self.num_heads\n",
        "\n",
        "\t\tself.query = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "\t\tself.key = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "\t\tself.value = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "\n",
        "\t\tself.linear_f = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "\n",
        "\tdef forward(self, q, k, v, mask=None):\n",
        "\t\tQ = self.query(q)\n",
        "\t\tK = self.key(k)\n",
        "\t\tV = self.value(v)\n",
        "\n",
        "\t\tQh = Q.view(Q.shape[0], Q.shape[1], self.num_heads, self.d_k)\n",
        "\t\tKh = K.view(K.shape[0], K.shape[1], self.num_heads, self.d_k)\n",
        "\t\tVh = V.view(V.shape[0], V.shape[1], self.num_heads, self.d_k)\n",
        "\n",
        "\t\t# (batch_size, max_length, num_heads, d_k) -> (batch_size, num_heads, max_length, d_k) | We do this in order to focus on each head\n",
        "\t\tQr = Qh.transpose(1, 2)\n",
        "\t\tKr = Kh.transpose(1, 2)\n",
        "\t\tVr = Vh.transpose(1, 2)\n",
        "\n",
        "\t\t# -- Scaled dot product --\n",
        "\t\t# (batch_size, num_heads, max_length, d_k) @ (batch_size, num_heads, d_k, max_length) -> (batch_size, num_heads, max_length, max_length)\n",
        "\n",
        "\t\tattention_scores = (torch.matmul(Qr, Kr.transpose(-2, -1)) / math.sqrt(self.d_k))\n",
        "\n",
        "\t\t# Apply the mask\n",
        "\t\tif mask is not None:\n",
        "\t\t\t# Apply the mask\n",
        "\t\t\t# mask = mask.unsqueeze(1)  # (batch_size, 1, seq_len, seq_len) or (batch_size, 1, 1, seq_len)\n",
        "            # Expand mask to all heads\n",
        "\n",
        "\t\t\tmask = mask.unsqueeze(dim=1).unsqueeze(dim=2)\n",
        "\t\t\t# print(attention_scores.shape)\n",
        "\t\t\t# print(mask.shape)\n",
        "\t\t\t# mask = mask.expand(q.shape[0], self.num_heads, *mask.shape[-2:])\n",
        "\t\t\tattention_scores = attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\t\t# Apply the softmax\n",
        "\t\tattention_scores = torch.softmax(input=attention_scores, dim=-1)\n",
        "\t\t# Apply the dropout\n",
        "\t\tattention_scores = self.dropout(attention_scores)\n",
        "\n",
        "\t\tattention_scores = torch.matmul(attention_scores, Vr)\n",
        "\n",
        "\t\tattention_scores = attention_scores.transpose(-2, -1).contiguous().view(Q.shape[0], Q.shape[1], self.d_model)\n",
        "\n",
        "\t\treturn self.linear_f(attention_scores)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "\tdef __init__(self, d_model : int, eps = 1e-5):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.eps = eps\n",
        "\t\tself.d_model = d_model\n",
        "\n",
        "\t\tself.gamma = nn.Parameter(torch.ones(self.d_model))\n",
        "\t\tself.beta = nn.Parameter(torch.zeros(self.d_model))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tmean = x.mean(dim=-1, keepdim=True)\n",
        "\t\tvar = x.var(dim=-1, keepdim=True)\n",
        "\n",
        "\t\tresult = (x - mean) / torch.sqrt(var ** 2 + self.eps)\n",
        "\n",
        "\t\treturn self.gamma * result + self.beta\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "\tdef __init__(self, d_model : int,  d_ff : int, dropout : float):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.d_model = d_model\n",
        "\t\tself.d_ff = d_ff\n",
        "\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\t\tself.linear_1 = nn.Linear(in_features=self.d_model, out_features=self.d_ff)\n",
        "\t\tself.relu = nn.ReLU()\n",
        "\t\tself.linear_2 = nn.Linear(in_features=self.d_ff, out_features=self.d_model)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.linear_2(self.dropout(self.relu(self.linear_1(x))))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\tdef __init__(self, d_model : int, num_heads : int, d_ff : int, dropout : float, max_length : int):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads, max_length=max_length, dropout=dropout)\n",
        "\t\tself.norm1 = LayerNormalization(d_model=d_model)\n",
        "\t\tself.feed_forward_network = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "\t\tself.norm2 = LayerNormalization(d_model=d_model)\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef forward(self, x, encoder_mask=None):\n",
        "\t\tattention_output = self.self_attention(\n",
        "\t\t\tq=x,\n",
        "\t\t\tk=x,\n",
        "\t\t\tv=x,\n",
        "\t\t\tmask=encoder_mask\n",
        "\t\t)\n",
        "\t\tx = x + self.dropout(attention_output)\n",
        "\t\tx = self.norm1(x)\n",
        "\n",
        "\t\tfeed_forward_output = self.feed_forward_network(x)\n",
        "\t\tx = x + self.dropout(feed_forward_output)\n",
        "\t\tx = self.norm2(x)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\tdef __init__(self, d_model : int, num_heads : int, d_ff : int, dropout : float, max_length : int):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads, max_length=max_length, dropout=dropout)\n",
        "\t\tself.norm1 = LayerNormalization(d_model=d_model)\n",
        "\t\tself.cross_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads, max_length=max_length, dropout=dropout)\n",
        "\t\tself.norm2 = LayerNormalization(d_model=d_model)\n",
        "\t\tself.feed_forward_network = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "\t\tself.norm3 = LayerNormalization(d_model=d_model)\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef forward(self, decoder_input, encoder_output, encoder_mask=None, decoder_mask=None):\n",
        "\t\t# Pass through the masked multi head attention\n",
        "\t\tmasked_attention_output = self.self_attention(\n",
        "\t\t\tq=decoder_input,\n",
        "\t\t\tk=decoder_input,\n",
        "\t\t\tv=decoder_input,\n",
        "\t\t\tmask=decoder_mask\n",
        "\t\t)\n",
        "\t\tdecoder_input = decoder_input + self.dropout(masked_attention_output)\n",
        "\t\tdecoder_input = self.norm1(decoder_input)\n",
        "\n",
        "\t\t# Pass the result through the cross attention\n",
        "\t\tcross_attention_output = self.cross_attention(\n",
        "\t\t\tq=decoder_input, # the decoder is trying to \"look up\" some information from the encoder\n",
        "\t\t\tk=encoder_output,\n",
        "\t\t\tv=encoder_output,\n",
        "\t\t\tmask=encoder_mask\n",
        "\t\t)\n",
        "\t\tdecoder_input = decoder_input + self.dropout(cross_attention_output)\n",
        "\t\tdecoder_input = self.norm2(decoder_input)\n",
        "\n",
        "\t\t# Pass the result through the Feed Forward network\n",
        "\t\tfeed_forward_output = self.feed_forward_network(decoder_input)\n",
        "\t\tdecoder_input = decoder_input + self.dropout(feed_forward_output)\n",
        "\n",
        "\t\tx = decoder_input\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\tdef __init__(self, d_model : int, encoder_vocab_size : int, decoder_vocab_size : int, max_length : int, num_heads : int, d_ff : int, dropout : float, N : int):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.encoder_embeddings = InputEmbedding(\n",
        "\t\t\td_model=d_model,\n",
        "\t\t\tvocab_size=encoder_vocab_size\n",
        "\t\t)\n",
        "\t\tself.decoder_embedding = InputEmbedding(\n",
        "\t\t\td_model=d_model,\n",
        "\t\t\tvocab_size=decoder_vocab_size\n",
        "\t\t)\n",
        "\t\tself.positional_encoding = PositionalEncoding(d_model=d_model, max_length=max_length)\n",
        "\t\tself.encoder_layers = nn.ModuleList([Encoder(d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout, max_length=max_length) for _ in range(N)])\n",
        "\t\tself.decoder_layers = nn.ModuleList([Decoder(\n",
        "\t\t\td_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout, max_length=max_length\n",
        "\t\t) for _ in range(N)])\n",
        "\t\tself.linear_projection =  nn.Linear(in_features=d_model, out_features=decoder_vocab_size)\n",
        "\n",
        "\tdef encode(self, encoder_input, encoder_mask):\n",
        "\t\tencoder_input = self.encoder_embeddings(encoder_input)\n",
        "\t\tencoder_input = self.positional_encoding(encoder_input)\n",
        "\t\tencoder_output = encoder_input\n",
        "\t\tfor layer in self.encoder_layers:\n",
        "\t\t\tencoder_output = layer(\n",
        "\t\t\t\tx=encoder_output,\n",
        "\t\t\t\tencoder_mask=encoder_mask\n",
        "\t\t\t)\n",
        "\n",
        "\t\treturn encoder_output\n",
        "\n",
        "\tdef decode(self, encoder_output, decoder_input, encoder_mask, decoder_mask):\n",
        "\t\tdecoder_input = self.decoder_embedding(decoder_input)\n",
        "\t\tdecoder_input = self.positional_encoding(decoder_input)\n",
        "\n",
        "\t\tdecoder_output = decoder_input\n",
        "\t\tfor layer in self.decoder_layers:\n",
        "\t\t\tdecoder_output = layer(\n",
        "\t\t\t decoder_input=decoder_output,\n",
        "\t\t\t encoder_output=encoder_output,\n",
        "\t\t\t encoder_mask=encoder_mask,\n",
        "\t\t\t decoder_mask=decoder_mask\n",
        "\t\t\t)\n",
        "\n",
        "\t\treturn decoder_output\n",
        "\n",
        "\tdef projection(self, x):\n",
        "\t\treturn self.linear_projection(x)\n",
        "\n",
        "\tdef forward(self, src, src_mask, tgt, tgt_mask):\n",
        "\t\t# Pass through the encoder\n",
        "\t\tencoder_output = self.encode(\n",
        "\t\t\tencoder_input=src,\n",
        "\t\t\tencoder_mask=src_mask\n",
        "\t\t)\n",
        "\n",
        "\t\t# Pass through the decoder\n",
        "\t\tdecoder_output = self.decode(\n",
        "\t\t\tencoder_output=encoder_output,\n",
        "\t\t\tdecoder_input=tgt,\n",
        "\t\t\tencoder_mask=src_mask,\n",
        "\t\t\tdecoder_mask=tgt_mask\n",
        "\t\t)\n",
        "\n",
        "\t\t# Pass through the projection layer\n",
        "\t\tproj = self.projection(decoder_output)\n",
        "\n",
        "\t\treturn proj"
      ],
      "metadata": {
        "id": "yAl7Mcr_Afh4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "import torch\n",
        "\n",
        "def build_transformer(config):\n",
        "\ttransformer = Transformer(\n",
        "\t\td_model=config[\"d_model\"],\n",
        "\t\tencoder_vocab_size=config[\"encoder_vocab_size\"],\n",
        "\t\tdecoder_vocab_size=config[\"decoder_vocab_size\"],\n",
        "\t\tmax_length=config[\"max_length\"],\n",
        "\t\tnum_heads=config[\"num_heads\"],\n",
        "\t\td_ff=config[\"d_ff\"],\n",
        "\t\tdropout=config[\"dropout\"],\n",
        "\t\tN=config[\"N\"]\n",
        "\t)\n",
        "\n",
        "\treturn transformer\n",
        "\n",
        "def build_tokenizer():\n",
        "\ttokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "\t# define the special tokens\n",
        "\tspecial_tokens = {\n",
        "\t\t\"additional_special_tokens\" : [\"<START>\", \"<END>\", \"<PAD>\"]\n",
        "\t}\n",
        "\ttokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "\treturn tokenizer"
      ],
      "metadata": {
        "id": "AkmkU8mvAitG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "\treturn {\n",
        "\t\t\"d_model\" : 512,\n",
        "\t\t\"encoder_vocab_size\" : 50265,\n",
        "\t\t\"decoder_vocab_size\" : 50265,\n",
        "\t\t\"max_length\" : 200,\n",
        "\t\t\"num_heads\" : 4,\n",
        "\t\t\"d_ff\" : 2048,\n",
        "\t\t\"dropout\" : 0.1,\n",
        "\t\t\"N\" : 8,\n",
        "\t\t\"epochs\" : 3\n",
        "\t}"
      ],
      "metadata": {
        "id": "ZN11vPjhCDBF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-qpqcmhd940K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "def train_model(config, writer, m=None):\n",
        "\tfrom dataset import load_dataset, create_dataloaders\n",
        "\t# Set up the device\n",
        "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\tprint(f\"Using {device} for training.\")\n",
        "\t# Set up the tokenizer\n",
        "\ttokenizer = build_tokenizer()\n",
        "\t# Get the config\n",
        "\tconfig = get_config()\n",
        "\t# Load the datasets\n",
        "\ttrain_data, test_data, validation_data = load_dataset()\n",
        "\n",
        "\ttrain_dataloader, test_dataloader, valid_dataloader = create_dataloaders(\n",
        "\t\ttrain_data=train_data,\n",
        "\t\ttest_data=test_data,\n",
        "\t\tvalidation_data=validation_data\n",
        "\t)\n",
        "\n",
        "\tmodel = build_transformer(config)\n",
        "\n",
        "\tmodel.to(device)\n",
        "\t# Define the loss function and optimizer\n",
        "\tloss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1).to(device)\n",
        "\n",
        "\tnum_epochs = 100\n",
        "\toptimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=1e-8)\n",
        "\tif m is not None:\n",
        "\t\tdata = torch.load(f=m)\n",
        "\t\tmodel_state_dict = data[\"model_state_dict\"]\n",
        "\t\tepoch = data[\"epoch\"] + 1\n",
        "\n",
        "\t\toptimizer_state_dict = data[\"optimizer_state_dict\"]\n",
        "\t\toptimizer.load_state_dict(optimizer_state_dict)\n",
        "\n",
        "\t\tmodel.load_state_dict(model_state_dict)\n",
        "\n",
        "\n",
        "\tscheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "\t\toptimizer,\n",
        "    max_lr=2e-5,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=len(train_dataloader),\n",
        "\t\tpct_start=0.1\n",
        "\t)\n",
        "\n",
        "\tearly_stopping_patience = 3\n",
        "\tearly_stopping_counter = 0\n",
        "\tbest_train_loss = float('inf')\n",
        "\n",
        "\tgradient_accumulation_steps = 2\n",
        "\n",
        "\tfor epoch in range(0, 50):\n",
        "\t\tbatch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch: {epoch}\")\n",
        "\t\tmodel.train()\n",
        "\t\ttrain_final_loss = 0\n",
        "\t\tstep = 0\n",
        "\t\tfor batch_idx, batch in enumerate(batch_iterator):\n",
        "\t\t\tencoder_input = batch[\"input_ids\"].to(device)\n",
        "\t\t\tencoder_mask = batch[\"attention_mask\"].to(device)\n",
        "\t\t\tdecoder_input = batch[\"decoder_input_ids\"].to(device)\n",
        "\t\t\tlabels = batch[\"labels\"].to(device)\n",
        "\t\t\tdecoder_mask = batch[\"decoder_attention_mask\"].to(device)\n",
        "\n",
        "\t\t\t# Do the forward pass\n",
        "\t\t\tpreds = model(\n",
        "\t\t\t\tsrc=encoder_input,\n",
        "\t\t\t\tsrc_mask=encoder_mask,\n",
        "\t\t\t\ttgt=decoder_input,\n",
        "\t\t\t\ttgt_mask=decoder_mask\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\tloss = loss_fn(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
        "\t\t\tbatch_iterator.set_postfix({f\"Loss\": f\"{loss:.2f}\"})\n",
        "\t\t\tloss.backward()\n",
        "\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tscheduler.step(loss)\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\tstep += 1\n",
        "\t\t\ttrain_final_loss += loss\n",
        "\n",
        "\t\t\tbatch_iterator.set_postfix({\n",
        "                \"Loss\": f\"{loss.item():.3f}\",\n",
        "                \"LR\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
        "            })\n",
        "\n",
        "\t\ttrain_final_loss /= step\n",
        "\t\t# writer.add_scalar(\"Training/Loss\", train_final_loss, epoch)\n",
        "\n",
        "\t\tprint(f\"\\nEpoch: {epoch} | Train Loss: {train_final_loss}\")\n",
        "\n",
        "\t\tif train_final_loss < best_val_loss:\n",
        "\t\t\tbest_val_loss = train_final_loss\n",
        "\t\t\tearly_stopping_counter = 0\n",
        "\t\t\tloss_for_model_name = math.floor(best_val_loss * 100)\n",
        "\t\t\tmodel_name = f\"me{epoch}l{loss_for_model_name}.pth\"\n",
        "\t\t\ttorch.save(\n",
        "\t\t\t\t\t obj={\n",
        "\t\t\t\t\t\t\t\t\"model_state_dict\": model.state_dict(),\n",
        "\t\t\t\t\t\t\t\t\"optimizer_state_dict\": optimizer.state_dict(),\n",
        "\t\t\t\t\t\t\t\t\"epoch\" : epoch,\n",
        "\t\t\t\t\t\t\t\t\"train_loss\" : train_final_loss\n",
        "\t\t\t\t\t },\n",
        "\t\t\t\t\tf=f\"models/{model_name}\"\n",
        "\t\t\t)\n",
        "\t\telse:\n",
        "\t\t\tearly_stopping_counter += 1\n",
        "\t\t\tif early_stopping_counter >= early_stopping_counter:\n",
        "\t\t\t\tprint(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\ttorch.cuda.empty_cache()\n",
        "\twriter.close()\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "HdaLVxbqHVbI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cZs2Nqgj940O"
      },
      "outputs": [],
      "source": [
        "cfg = get_config()\n",
        "writer = SummaryWriter()\n",
        "m = Path(\"/content/models/me14l177.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE2utNDbC9z3",
        "outputId": "33b70962-bb7b-4770-805d-bf5afc94c245"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d_model': 512,\n",
              " 'encoder_vocab_size': 50265,\n",
              " 'decoder_vocab_size': 50265,\n",
              " 'max_length': 200,\n",
              " 'num_heads': 4,\n",
              " 'd_ff': 2048,\n",
              " 'dropout': 0.1,\n",
              " 'N': 8,\n",
              " 'epochs': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB0ZwDJyB7ey",
        "outputId": "c60032bd-f879-4743-90f0-7661b38acd1b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.tensorboard.writer.SummaryWriter at 0x7f173e2753d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v97srVZzG0nj",
        "outputId": "bf7bf735-7d5a-4f59-b8a1-0b9214d38511"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/models/me14l177.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uVjvZjeKHphb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(cfg, writer, m=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "u-H8izVZC_X1",
        "outputId": "e6c45384-638d-45e6-e631-57058453b727"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda for training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch: 0:   0%|          | 0/348 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 35.06 MiB is free. Process 9531 has 14.71 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 258.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-685ecb1185fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-3654d2fc1848>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config, writer, m)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0;31m# Do the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \t\t\tpreds = model(\n\u001b[0m\u001b[1;32m     72\u001b[0m                                 \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                 \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0f8e9dd13217>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;31m# Pass through the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m \t\tencoder_output = self.encode(\n\u001b[0m\u001b[1;32m    240\u001b[0m                         \u001b[0mencoder_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                         \u001b[0mencoder_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0f8e9dd13217>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, encoder_input, encoder_mask)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \t\t\tencoder_output = layer(\n\u001b[0m\u001b[1;32m    213\u001b[0m                                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                                 \u001b[0mencoder_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0f8e9dd13217>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_mask)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mfeed_forward_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_forward_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0f8e9dd13217>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 35.06 MiB is free. Process 9531 has 14.71 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 258.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8a3jAY5O0KqJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}